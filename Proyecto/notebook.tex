
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Credit Approval}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{quote}
\textbf{Alumnos}: \emph{Adrián Ogáyar Sanchez y Arturo Barbero Pérez}
\end{quote}

\begin{quote}
\textbf{Grupo}: \emph{11}
\end{quote}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \section{ÍNDICE}\label{uxedndice}

\begin{itemize}
\item
  INTRODUCCIÓN
\item
  VISUALIZACIÓN DE LOS DATOS

  \begin{itemize}
  \item
    Variables desaparecidas
  \item
    Sustitución de las variables desaparecidas

    \begin{itemize}
    \tightlist
    \item
      Variables numéricas
    \item
      Variables categóricas
    \item
      Comprobando fiabilidad de los datos

      \begin{itemize}
      \tightlist
      \item
        Ajustar Lambda
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  REGRESIÓN LOGÍSTICA
\item
  REDES NEURONALES

  \begin{itemize}
  \tightlist
  \item
    Hipótesis
  \item
    Función de coste
  \item
    Gradiente
  \end{itemize}
\item
  SUPPORT VECTOR MACHINES

  \begin{itemize}
  \tightlist
  \item
    Kernel
  \item
    Parámetro de regulación \emph{C}
  \end{itemize}
\item
  CONCLUSIONES
\item
  BIBLIOGRAFÍA
\end{itemize}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \section{INTRODUCCIÓN}\label{introducciuxf3n}

    El objetivo de este proyecto es aplicar los conocimientos aprendidos en
la asignatura para crear un sistema de aprendizaje automático que nos
permita clasificar un conjunto de datos. Para ello aplicaremos distintos
modelos de entrenamiento y analizaremos su rendimiento.

El dataset consiste en información de créditos bancarios de una compañía
japonesa, se disponen de 690 casos, que se dividen en aprobados y
rechazados, con 307 casos positivos (aprobados) y 383 casos negativos
(rechazados). Se disponen de 15 atributos, cuyos nombres y valores han
sido cambiados para proteger la confidencialidad de la información.

    Comenzamos importando las librerías que van a ser necesarias durante el
desarrollo del proyecto: La librería \textbf{Pyplot} es necesaría para
poder hacer el graficado de los números contenidos en la matriz de
datos. \textbf{NumPy} es la librería que nos permite realizar calculos
entre matrices y vectores de manera de más eficiente gracias a la
vectorización. Importamos, además, \textbf{Pandas} que proporciona
estructuras de datos de alto rendimiento, y herramientas de análisis de
datos. Por último, añadimos \textbf{Optimize} que nos permitiría
entrenar nuestros modelos.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{optimize} \PY{k}{as} \PY{n+nn}{opt}
\end{Verbatim}


    \section{VISUALIZACIÓN DE LOS
DATOS}\label{visualizaciuxf3n-de-los-datos}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{target\PYZus{}url} \PY{o}{=} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{http://archive.ics.uci.edu/ml/machine\PYZhy{}learning\PYZhy{}databases}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}
                      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/credit\PYZhy{}screening/crx.data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{header} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A4}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A6}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A7}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A8}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A9}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A10}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A11}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A12}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A13}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A14}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A15}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A16}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{types} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float64}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}
        \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{target\PYZus{}url}\PY{p}{,} \PY{n}{names}\PY{o}{=}\PY{n}{header}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(690, 16)

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:}    A1     A2      A3 A4 A5  A6 A7     A8 A9 A10  A11 A12 A13    A14    A15 A16
        0   b  30.83   0.000  u  g   w  v  1.250  t   t    1   f   g  00202      0   +
        1   a  58.67   4.460  u  g   q  h  3.040  t   t    6   f   g  00043    560   +
        2   a  24.50   0.500  u  g   q  h  1.500  t   f    0   f   g  00280    824   +
        3   b  27.83   1.540  u  g   w  v  3.750  t   t    5   t   g  00100      3   +
        4   b  20.17   5.625  u  g   w  v  1.710  t   f    0   f   s  00120      0   +
        5   b  32.08   4.000  u  g   m  v  2.500  t   f    0   t   g  00360      0   +
        6   b  33.17   1.040  u  g   r  h  6.500  t   f    0   t   g  00164  31285   +
        7   a  22.92  11.585  u  g  cc  v  0.040  t   f    0   f   g  00080   1349   +
        8   b  54.42   0.500  y  p   k  h  3.960  t   f    0   f   g  00180    314   +
        9   b  42.50   4.915  y  p   w  v  3.165  t   f    0   t   g  00052   1442   +
        10  b  22.08   0.830  u  g   c  h  2.165  f   f    0   t   g  00128      0   +
        11  b  29.92   1.835  u  g   c  h  4.335  t   f    0   f   g  00260    200   +
        12  a  38.25   6.000  u  g   k  v  1.000  t   f    0   t   g  00000      0   +
        13  b  48.08   6.040  u  g   k  v  0.040  f   f    0   f   g  00000   2690   +
        14  a  45.83  10.500  u  g   q  v  5.000  t   t    7   t   g  00000      0   +
        15  b  36.67   4.415  y  p   k  v  0.250  t   t   10   t   g  00320      0   +
        16  b  28.25   0.875  u  g   m  v  0.960  t   t    3   t   g  00396      0   +
        17  a  23.25   5.875  u  g   q  v  3.170  t   t   10   f   g  00120    245   +
        18  b  21.83   0.250  u  g   d  h  0.665  t   f    0   t   g  00000      0   +
        19  a  19.17   8.585  u  g  cc  h  0.750  t   t    7   f   g  00096      0   +
        20  b  25.00  11.250  u  g   c  v  2.500  t   t   17   f   g  00200   1208   +
        21  b  23.25   1.000  u  g   c  v  0.835  t   f    0   f   s  00300      0   +
        22  a  47.75   8.000  u  g   c  v  7.875  t   t    6   t   g  00000   1260   +
        23  a  27.42  14.500  u  g   x  h  3.085  t   t    1   f   g  00120     11   +
        24  a  41.17   6.500  u  g   q  v  0.500  t   t    3   t   g  00145      0   +
\end{Verbatim}
            
    \subsection{VARIABLES DESAPARECIDAS}\label{variables-desaparecidas}

    Uno de los problemas que nos encontramos es que los datos están
compuestos por números y letras, por lo que en este estado resulta
imposible operar con ellos. Para poder utilizarlos usamos el metodo de
\emph{One Hot Encode}, de modo que las variables con strings se dividen
múltiples variables que contienen los posibles valores de la variable
inicial con un 1 si el caso tiene ese string o 0 si no. Con
\emph{pandas.get\_dummies} podemos realizar esta operación de forma
automática.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{dataDummie} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n}{dataDummie}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:}       A3    A8  A11  A15  A1\_?  A1\_a  A1\_b  A2\_13.75  A2\_15.17  A2\_15.75  \textbackslash{}
        0  0.000  1.25    1    0     0     0     1         0         0         0   
        1  4.460  3.04    6  560     0     1     0         0         0         0   
        2  0.500  1.50    0  824     0     1     0         0         0         0   
        3  1.540  3.75    5    3     0     0     1         0         0         0   
        4  5.625  1.71    0    0     0     0     1         0         0         0   
        
           {\ldots}    A14\_00720  A14\_00760  A14\_00840  A14\_00928  A14\_00980  A14\_01160  \textbackslash{}
        0  {\ldots}            0          0          0          0          0          0   
        1  {\ldots}            0          0          0          0          0          0   
        2  {\ldots}            0          0          0          0          0          0   
        3  {\ldots}            0          0          0          0          0          0   
        4  {\ldots}            0          0          0          0          0          0   
        
           A14\_02000  A14\_?  A16\_+  A16\_-  
        0          0      0      1      0  
        1          0      0      1      0  
        2          0      0      1      0  
        3          0      0      1      0  
        4          0      0      1      0  
        
        [5 rows x 572 columns]
\end{Verbatim}
            
    Como se puede observar hemos pasado de 16 columnas (variables) a 572. ¿A
qué se debe esto? Si comprobamos el tipo de los datos que tenemos
podemos ver que algunos que deberían ser de tipo númerico aparecen como
object (string).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{data}\PY{o}{.}\PY{n}{ftypes}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} A1      object:dense
        A2      object:dense
        A3     float64:dense
        A4      object:dense
        A5      object:dense
        A6      object:dense
        A7      object:dense
        A8     float64:dense
        A9      object:dense
        A10     object:dense
        A11      int64:dense
        A12     object:dense
        A13     object:dense
        A14     object:dense
        A15      int64:dense
        A16     object:dense
        dtype: object
\end{Verbatim}
            
    Este problema genera a su vez otro: Los datos numéricos que son tomados
como strings hacen que el One Hot Encoding genere una nueva variable por
cada número diferente que se encuentra en la variable inicial.

Empezaremos por convertir la última variable (A16), que representa los
créditos aprobados con un '+' y los denegados con un '-', en un valor
binario 1 en caso de que esté aprobado y 0 en caso contrario.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{data}\PY{o}{.}\PY{n}{A16} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1} \PY{k}{if} \PY{n}{d}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{+}\PY{l+s+s1}{\PYZsq{}} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A16}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
\end{Verbatim}


    Sin embargo hay otras variables que deberían ser numéricas y aparecen
como un string. Esto nos lleva a otra pregunta. ¿Por que se leen como
string? La respuesta es sencilla: Las variables desaparecidas. El
conjunto de datos no está completo, algunos casos no tienen datos para
una o varias de sus variables, y para representar estos valores
desconocidos se usa un '?'. Esto hace que en la lectura se detecte la
columna como un string.

    Utilizando el paquete https://github.com/ResidentMario/missingno podemos
mostrar la distribución de las variables desaparecidas. Para ello es
necesario que antes sustituyamos los '?' por el valor 'NaN'.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{import} \PY{n+nn}{missingno}\PY{n+nn}{.}\PY{n+nn}{missingno} \PY{k}{as} \PY{n+nn}{msno}
        \PY{n}{d1} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{d1}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{msno}\PY{o}{.}\PY{n}{matrix}\PY{p}{(}\PY{n}{d1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A1     12
A2     12
A3      0
A4      6
A5      6
A6      9
A7      9
A8      0
A9      0
A10     0
A11     0
A12     0
A13     0
A14    13
A15     0
A16     0
dtype: int64

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x284029d3908>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Como se puede ver los datos desconocidos solo se encuentran en las
variables \emph{A1, A2, A4, A5, A6, A7 y A14} y en algunos casos no
contamos con ningún dato entre \emph{A4} y \emph{A7}. Esto hace que sea
muy difícil utilizar estos casos para el entrenamiento. Una opción sería
descartar estos casos (en especial aquellos con demasiadas variables
desaparecidas), sin embargo en Machine Learning es importante disponer
de la mayor cantidad de datos posible y en nuestro caso contamos con un
número muy reducido (690 casos), por lo que usaremos otros métodos para
aprovechar los datos de los casos con variables desaparecidas.

    \subsection{SUSTITUCIÓN DE LAS VARIABLES
DESAPARECIDAS}\label{sustituciuxf3n-de-las-variables-desaparecidas}

    Aquí habría que realizar un análsis de los datos númericos, con gráficas
a ser posible y comprobar si hay anomalias, en caso de que las haya
habría que sustituir por la mediana, en caso contrario, por la media.\\
Las variables categoricas habría que sustituirlas por el valor más
repetido (moda) ó crear una nueva clase llamada '?', dependiendo de cada
caso.

    Para poder utilizar los casos con variables desaparecidas utilizaremos
distintos métodos y los analizaremos para quedarnos con el que nos
ofrezca mejor rendimiento.

    \subsubsection{VARIABLES NUMÉRICAS}\label{variables-numuxe9ricas}

    Comezamos analizando los datos de las variables numéricas con datos
incompletos, es decir, la columna A2 y A14. Con esto podremos observar
su comportamiento para así saber la mejor forma de tratar a las
variables desaparecidas.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{}Eliminamos los casos con variables desaparecidas}
        \PY{n}{d2} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}Cambiamos el tipo de A2 a numérico. Pasa a float o int según los datos almacenados.}
        \PY{c+c1}{\PYZsh{}Si hay un string salta un error.}
        \PY{n}{d2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}numeric}\PY{p}{(}\PY{n}{d2}\PY{o}{.}\PY{n}{A2}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}Graficamos los datos}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{d2}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A2}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Media A2: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d2}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mediana A2: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d2}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Media A2: 31.50381316998472
Mediana A2: 28.42

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    En este caso, puesto que no hay muchas anomalías en los datos, es más
recomendable que sustituyamos las variables desaparecidas por la media
de estos.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{d2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A14}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}numeric}\PY{p}{(}\PY{n}{d2}\PY{o}{.}\PY{n}{A14}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{d2}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A14}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Media A14: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d2}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mediana A14: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d2}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Media A14: 180.35987748851454
Mediana A14: 160.0

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_28_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Sin embargo, en este caso, es menos recomendable que sustituyamos con la
media puesto que hay ciertas anomalías en los datos llegando alguno de
ellos incluso a 1000 ó 2000 en el eje de las \emph{y}, cuando la mayoría
de estos tienen unos valores mucho más bajos.

Comprobamos como han cambiado los datos si sustituimos A2 con la media:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{d3} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A2} \PY{o}{=} \PY{n}{d3}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A2} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}numeric}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A2}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{d3}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{d3}\PY{o}{.}\PY{n}{A2}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Reemplazando los valores desaparecidos con la media:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Media A2: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mediana A2: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Diferencia entre las medias A2: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{d2}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Diferencia entre las medianas A2: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{d2}\PY{o}{.}\PY{n}{A2}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Reemplazando los valores desaparecidos con la media:
Media A2: 31.567051823246135
Mediana A2: 28.625

Diferencia entre las medias A2: 0.06323865326141487
Diferencia entre las medianas A2: 0.2049999999999983

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Comprobamos como han cambiado los datos si sustituimos ahora A14 por la
mediana:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{d3}\PY{o}{.}\PY{n}{A14} \PY{o}{=} \PY{n}{d3}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A14} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}numeric}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A14}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{d3}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{d3}\PY{o}{.}\PY{n}{A14}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Reemplazando los valores desaparecidos con la mediana:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Media A14: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mediana A14: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Diferencia entre las medias A2: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{d2}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Diferencia entre las medianas A2: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{d3}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{d2}\PY{o}{.}\PY{n}{A14}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Reemplazando los valores desaparecidos con la mediana:
Media A14: 183.5623188405797
Mediana A14: 160.0

Diferencia entre las medias A2: 3.202441352065165
Diferencia entre las medianas A2: 0.0

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{VARIABLES CATEGÓRICAS}\label{variables-categuxf3ricas}

    Seguimos analizando los datos de las variables categóricas con datos
incompletos, es decir, las características A1, A4, A5, A6 y A7.

La sustitución de estos datos podríamos realizarla de dos maneras: La
primera y más lógica sería sustituir los valores desaparecidos por la
moda, es decir, el valor que se repita con mayor frecuencia en cada
columna. Esto puede ser contraproducente dependiendo de cuántos valores
desaparecidos tenga la columna. Si tiene muchos, podríamos realizarla de
una segunda manera, basada en crear una nueva clase para estos valores
que faltan. Esta estrategia agregará más información al conjunto de
datos, lo que dará como resultado un cambio de varianza posterior,
cuando tengamos que entrenar nuestro sistema. Por lo tanto, tenemos que
ver que caso nos conviene más para cada característica:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A1

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} b    468
         a    210
         ?     12
         Name: A1, dtype: int64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A4

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:} u    519
         y    163
         ?      6
         l      2
         Name: A4, dtype: int64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A5

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} g     519
         p     163
         ?       6
         gg      2
         Name: A5, dtype: int64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A6}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A6}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A6

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} c     137
         q      78
         w      64
         i      59
         aa     54
         ff     53
         k      51
         cc     41
         m      38
         x      38
         d      30
         e      25
         j      10
         ?       9
         r       3
         Name: A6, dtype: int64
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A7}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A7}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A7

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} v     399
         h     138
         bb     59
         ff     57
         ?       9
         z       8
         j       8
         dd      6
         n       4
         o       2
         Name: A7, dtype: int64
\end{Verbatim}
            
    Dado que el número de variables desaparecidas está alrededor de las
6-12, y el número de variables más frecuentes para cada característica
sobrepasa en la mayoría de los casos los 400, se va a optar por sustiuir
todas estas con la moda, es decir, el valor más frecuente para cada
característica.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{d3}\PY{o}{.}\PY{n}{A1} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A1}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A4} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A4}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A5} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A5}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A6} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A6}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A7} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A7}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    Por tanto, una vez hemos remplazado estos valores, volvemos a mostrar
como quedaría nuestro conjunto de datos:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{d4} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{d3}\PY{p}{)}
         \PY{n}{d4}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:}       A2     A3    A8  A11    A14  A15  A16  A1\_a  A1\_b  A4\_a  {\ldots}    A7\_b  \textbackslash{}
         0  30.83  0.000  1.25    1  202.0    0    1     0     1     0  {\ldots}       1   
         1  58.67  4.460  3.04    6   43.0  560    1     1     0     1  {\ldots}       0   
         2  24.50  0.500  1.50    0  280.0  824    1     1     0     1  {\ldots}       0   
         3  27.83  1.540  3.75    5  100.0    3    1     0     1     0  {\ldots}       1   
         4  20.17  5.625  1.71    0  120.0    0    1     0     1     0  {\ldots}       1   
         
            A9\_f  A9\_t  A10\_f  A10\_t  A12\_f  A12\_t  A13\_g  A13\_p  A13\_s  
         0     0     1      0      1      1      0      1      0      0  
         1     0     1      0      1      1      0      1      0      0  
         2     0     1      1      0      1      0      1      0      0  
         3     0     1      0      1      0      1      1      0      0  
         4     0     1      1      0      1      0      0      0      1  
         
         [5 rows x 26 columns]
\end{Verbatim}
            
    Cuyas columnas son las siguientes:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{d4}\PY{o}{.}\PY{n}{columns}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} Index(['A2', 'A3', 'A8', 'A11', 'A14', 'A15', 'A16', 'A1\_a', 'A1\_b', 'A4\_a',
                'A4\_b', 'A5\_a', 'A5\_b', 'A6\_a', 'A6\_b', 'A7\_a', 'A7\_b', 'A9\_f', 'A9\_t',
                'A10\_f', 'A10\_t', 'A12\_f', 'A12\_t', 'A13\_g', 'A13\_p', 'A13\_s'],
               dtype='object')
\end{Verbatim}
            
    \subsubsection{COMPROBANDO LA FIABILIDAD DE LOS
DATOS}\label{comprobando-la-fiabilidad-de-los-datos}

    Para asegurarnos de que esta configuración de nuestros datos nos ofrece
un buen rendimiento, vamos a realizar un primer entrenamiento con el
modelo de \emph{Regresión Logística}, pues ofrece un rendimiento medio y
es el más rápido lo que nos permitirá realizar más pruebas.

Por tanto, comenzamos con este modelo de entrenamiento:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split} \PY{k}{as} \PY{n}{tts}
\end{Verbatim}


    Importamos la función \emph{train\_test\_split}, de \emph{sklearn}, que
nos va a permitir separar nuestro conjunto de datos en dos: Uno para
entrenar y otro para validar. Con este último mediremos el error de
predicción que ofrece nuestro modelo.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{x} \PY{o}{=} \PY{n}{d4}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{d4}\PY{o}{.}\PY{n}{columns}\PY{o}{!=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A16}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{y} \PY{o}{=} \PY{n}{d4}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A16}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    Dividiremos los datos en un 80\% (552) de entrenamiento y un 20\% (138)
de validación pues nuestro conjunto de datos es pequeño y es necesario
conservar la mayor cantidad de datos posible para el entrenamiento.
Usamos un 20\% en lugar de un 10\% debido a que el 10\% (69 casos) es
una cantidad demasiado pequeña para que los porcentajes sean fiables (un
caso más o menos clasificado correctamente hace variar el porcentaje
total en 1.5\%).

Además, la división debe de ser aleatoria, pues algunos casos pueden
entrenar mejor que otros, y seleccionar arbitrariamente los casos podría
dar lugar a un resultado que no es realista.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{xTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{yVal} \PY{o}{=} \PY{n}{tts}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Porcentaje de casos de validación positivos respecto de los negativos}
         \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{yVal}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{yVal}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} 55.072463768115945
\end{Verbatim}
            
    Pasamos entonces a entrenar nuestro modelo. Para ello definimos las
funciones que utilizará la regresión lineal. Estas funciones se
explicarán en el apartado correspondiente de \emph{Regresión Logística}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{g} \PY{o}{=} \PY{k}{lambda} \PY{n}{z}\PY{p}{:} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{e}\PY{o}{*}\PY{o}{*}\PY{o}{\PYZhy{}}\PY{n}{z}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{safe\PYZus{}ln}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{minval}\PY{o}{=}\PY{l+m+mf}{0.0000000001}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{clip}\PY{p}{(}\PY{n+nb}{min}\PY{o}{=}\PY{n}{minval}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{gradient}\PY{p}{(}\PY{n}{theta}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}\PY{p}{:}
             \PY{n}{grad} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{g}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{theta}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)}
             \PY{n}{grad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{lamb}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
             \PY{k}{return} \PY{n}{grad}
         
         \PY{k}{def} \PY{n+nf}{funCoste}\PY{p}{(}\PY{n}{theta}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{p}{(}\PY{p}{(}\PY{n}{safe\PYZus{}ln}\PY{p}{(} \PY{n}{g}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{theta}\PY{p}{)}\PY{p}{)} \PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{y}\PY{p}{)} \PY{o}{+}
                                 \PY{p}{(}\PY{p}{(}\PY{n}{safe\PYZus{}ln}\PY{p}{(} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{g}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{theta}\PY{p}{)}\PY{p}{)} \PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{y}\PY{p}{)} \PY{o}{+}
                                 \PY{p}{(}\PY{n}{lamb}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{normalizar}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{p}{:}
             \PY{n}{mu} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
             \PY{n}{s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
             
             \PY{n}{x} \PY{o}{=} \PY{p}{(}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{mu}\PY{p}{)}\PY{o}{/}\PY{n}{s}
             
             \PY{k}{return} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{s}\PY{p}{)}
\end{Verbatim}


    Definimos una función que entrene a partir de unos datos dados y nos
devuelva el porcentaje de acierto de la validación, también haremos que
devuelva el porcentaje de acierto sobre los propios casos de
entrenamiento, que nos permitirá ver si existe sobreajuste.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{k}{def} \PY{n+nf}{trainRegresion}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{n}{lamb}\PY{p}{)} \PY{p}{:}
             \PY{n}{theta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{result} \PY{o}{=} \PY{n}{opt}\PY{o}{.}\PY{n}{fmin\PYZus{}tnc}\PY{p}{(}\PY{n}{func}\PY{o}{=}\PY{n}{funCoste}\PY{p}{,} \PY{n}{x0}\PY{o}{=}\PY{n}{theta}\PY{p}{,} 
                                   \PY{n}{fprime}\PY{o}{=}\PY{n}{gradient}\PY{p}{,} \PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}\PY{p}{)}
             \PY{n}{thetaOpts} \PY{o}{=} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
             \PY{n}{yResult} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1} \PY{k}{if} \PY{n}{i} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.5} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{g}\PY{p}{(}\PY{n}{xVal}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{thetaOpts}\PY{p}{)}\PY{p}{)}\PY{p}{]}
             \PY{n}{valP} \PY{o}{=} \PY{p}{(}\PY{n}{yVal}\PY{o}{==}\PY{n}{yResult}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{yVal}\PY{p}{)}
             
             \PY{n}{yResult} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1} \PY{k}{if} \PY{n}{i} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.5} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{g}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{thetaOpts}\PY{p}{)}\PY{p}{)}\PY{p}{]}
             \PY{n}{trainP} \PY{o}{=} \PY{p}{(}\PY{n}{y}\PY{o}{==}\PY{n}{yResult}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}
             
             \PY{k}{return}\PY{p}{(}\PY{n}{valP} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{trainP} \PY{o}{*} \PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{trainRegresion}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}25}]:} (86.23188405797102, 86.59420289855072)
\end{Verbatim}
            
    Estos porcentajes variarán según la división de los datos, pero siempre
rondará el 85\% tanto en la validación como en el entrenamiento. También
se puede comprobar realizando la media de multiples entrenamientos.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k}{def} \PY{n+nf}{meanTrainRegresion}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{lamb}\PY{p}{,} \PY{n}{iteraciones}\PY{p}{)} \PY{p}{:} 
             \PY{n}{train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{val} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{iteraciones}\PY{p}{)} \PY{p}{:}
                 \PY{n}{xTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{yVal} \PY{o}{=} \PY{n}{tts}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
                 
                 \PY{n}{result} \PY{o}{=} \PY{n}{trainRegresion}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}
                 
                 \PY{n}{val}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{result}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} 
                 \PY{n}{train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{result}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                 
             \PY{k}{return} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{val}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{meanTrainRegresion}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} (85.12753623188406, 86.31920289855074)
\end{Verbatim}
            
    De este modo podemos observar que el modelo acierta casi el 85\% de los
casos de validación y algo más del 86\% de los casos de entrenamiento.
Estas cifras podrían considerarse aceptables para considerar que la
clasificación es correcta, sin embargo intentaremos ajustar el modelo
buscando unos mejores porcentajes.

Ese 86\% de casos de entrenamiento clasificados correctamente nos indica
que podría haber un pequeño \emph{underfitting}, es decir, que el modelo
no se ajusta del todo a los casos de entrenamiento. Nunca es deseable
que ese porcentaje sea del 100\% (y en muchos casos es practicamente
imposible) pues eso nos llevaría a un problema de \emph{overfitting},
pero intentaremos aplicar medidas contra el \emph{underfitting} para
comprobar si los porcentajes mejoran.

Crearemos una función que nos permita ver las curvas de aprendizaje para
analizar la bias y la varianza, a esta función le agregaremos un
parámetro que nos permita seleccionar el tramo a mostrar para facilitar
el análisis:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{k}{def} \PY{n+nf}{curvaAprendizaje}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{xval}\PY{p}{,} \PY{n}{yval}\PY{p}{,} \PY{n}{lamb}\PY{p}{,} \PY{n}{ret}\PY{p}{,} \PY{n}{intervalo}\PY{o}{=}\PY{n+nb}{slice}\PY{p}{(}\PY{k+kc}{None}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{trainError} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{validationError} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 
                 \PY{n}{theta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n}{norm} \PY{o}{=} \PY{n}{normalizar}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{norm}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                 \PY{n}{result} \PY{o}{=} \PY{n}{opt}\PY{o}{.}\PY{n}{fmin\PYZus{}tnc}\PY{p}{(}\PY{n}{func}\PY{o}{=}\PY{n}{funCoste}\PY{p}{,} \PY{n}{x0}\PY{o}{=}\PY{n}{theta}\PY{p}{,} \PY{n}{fprime}\PY{o}{=}\PY{n}{gradient}\PY{p}{,} 
                                       \PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}\PY{p}{)}
                 \PY{n}{thetaOpts} \PY{o}{=} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
           
                 \PY{n}{trainError}\PY{p}{[}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{funCoste}\PY{p}{(}\PY{n}{thetaOpts}\PY{p}{,} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}
          
                 \PY{n}{xval} \PY{o}{=} \PY{p}{(}\PY{n}{xval}\PY{o}{\PYZhy{}}\PY{n}{norm}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{/}\PY{n}{norm}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
                 \PY{n}{validationError}\PY{p}{[}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{funCoste}\PY{p}{(}\PY{n}{thetaOpts}\PY{p}{,} \PY{n}{xval}\PY{p}{,} \PY{n}{yval}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}
                 
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{xTrain}\PY{p}{[}\PY{n}{intervalo}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{trainError}\PY{p}{[}\PY{n}{intervalo}\PY{p}{]}\PY{p}{,} 
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Entrenamiento}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{xTrain}\PY{p}{[}\PY{n}{intervalo}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{validationError}\PY{p}{[}\PY{n}{intervalo}\PY{p}{]}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validación}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Número de ejemplos de entrenamiento}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
             
             \PY{k}{if}\PY{p}{(}\PY{n}{ret} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:} \PY{k}{return} \PY{p}{(}\PY{n}{trainError}\PY{p}{,} \PY{n}{validationError}\PY{p}{)}
             \PY{k}{else}\PY{p}{:} \PY{k}{return}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{np}\PY{o}{.}\PY{n}{seterr}\PY{p}{(}\PY{n}{over}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{tError}\PY{p}{,} \PY{n}{valError} \PY{o}{=} \PY{n}{curvaAprendizaje}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_63_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{a} \PY{o}{=} \PY{p}{(}\PY{n}{valError} \PY{o}{\PYZhy{}} \PY{n}{tError}\PY{p}{)}
         \PY{n}{valor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{a} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.75}\PY{p}{)}
         \PY{n}{valor}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:} 57
\end{Verbatim}
            
    Para analizar mejor las curvas mostraremos los ejemplos de entrenamiento
a partir de \emph{valor}, que es donde las curvas se empiezan a juntar.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{curvaAprendizaje}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{slice}\PY{p}{(}\PY{n}{valor}\PY{p}{,}\PY{k+kc}{None}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Como se puede ver las curvas apenas tienen separación e incluso pueden
llegar a juntarse (según los valores de entrenamiento/validación
obtenidos). Por tanto podemos afirmar que el modelo no tiene mucha
varianza y no hay \emph{overfitting}.

Del mismo modo el coste final de las curvas es lo bastante bajo como
para confirmar que tampoco hay demasiada bias y se podría afirmar que no
existe \emph{underfitting}.

Sin embargo como ya hemos dicho intentaremos mejorar el modelo para
comprobar si es posible un mejor resultado.

    \paragraph{\texorpdfstring{AJUSTAR
\(\lambda\)}{AJUSTAR \textbackslash{}lambda}}\label{ajustar-lambda}

    Una buena forma de solucionar problemas de \emph{underfitting} y
\emph{overfitting} es ajustar la variable \(\lambda\), si nuestra
\(\lambda\) es demasiado grande probablemente nos lleve a un problema de
\emph{underfitting}, mientras que si es muy pequeña será de
\emph{overfitting}, por tanto, es importante encontrar un \(\lambda\)
óptima para nuestro modelo.

Para ello vamos a generar una secuencia de 2000 lambdas, que oscilen
entre 0 y 100

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{n}{lambdas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{2000}\PY{p}{)}
\end{Verbatim}


    Con estas lambdas vamos a entrenar nuestro modelo y lo evaluaremos con
los datos de validación. Seleccionaremos aquella lambda que máximice el
porcentaje de acierto sobre los ejemplos de entrenamiento.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{mejor} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{lambdas}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{mejor}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{trainRegresion}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{n}{lambdas}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{mejor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{mejor}\PY{p}{)}
         \PY{n}{l} \PY{o}{=} \PY{n}{lambdas}\PY{p}{[}\PY{n}{mejor}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{p}{)}\PY{p}{]}
         \PY{n}{l}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}34}]:} 0.3001500750375187
\end{Verbatim}
            
    Como podemos ver, el porcentaje de aciertos mejora en cierta medida
gracias al \(\lambda\) obtenido.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{trainRegresion}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{n}{l}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}35}]:} (86.95652173913044, 86.77536231884058)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{media1} \PY{o}{=} \PY{n}{meanTrainRegresion}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{l}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{media1}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} 84.82028985507246
\end{Verbatim}
            
    Si realizamos la curva de aprendizaje en el mismo punto donde antes se
juntaban las curvas vemos que hay un menor \emph{gap} entre ellas dos,
indicando esto que los datos de validación son capaces de adaptarse
mejor a los de entrenamiento.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{curvaAprendizaje}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{slice}\PY{p}{(}\PY{n}{valor}\PY{p}{,}\PY{k+kc}{None}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_78_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{curvaAprendizaje}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{n}{l}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{slice}\PY{p}{(}\PY{n}{valor}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_79_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Estos son los resultados obtenidos con las variables categóricas
desaparecidas sustituidas mediante la moda. Comprobaremos ahora los
resultados con la siguiente configuración.

Como vemos en la distribución de las variables categóricas, la variable
A1 tiene tres posibles valores: a, b y ? Dado que esta ultima aparece
solo 12 veces en comparación con los otros dos valores (210 y 468
respectivamente), seguiremos sustiyendola por la moda. Sin embargo, para
el resto crearemos una nueva clase llamada '?' dado que para todas ellas
hay valores que aparecen con menos frecuencia y se han considerado como
una clase.

Antes de nada, crearemos una función que realice lo necesario para
ajustar \(\lambda\) para tener obtener conclusiones de ello después:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{k}{def} \PY{n+nf}{ajustarLamb}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{lambdas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{2000}\PY{p}{)}
             \PY{n}{mejor} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{lambdas}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{mejor}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{trainRegresion}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{n}{lambdas}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                 
             \PY{n}{mejor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{mejor}\PY{p}{)}
             \PY{n}{l} \PY{o}{=} \PY{n}{lambdas}\PY{p}{[}\PY{n}{mejor}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{p}{)}\PY{p}{]}
             
             \PY{k}{return} \PY{n}{l}
\end{Verbatim}


    Remplazamos los datos con la nueva configuración:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{d3}\PY{o}{.}\PY{n}{A1} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{d2}\PY{o}{.}\PY{n}{A1}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A4} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A5} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A6} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{d3}\PY{o}{.}\PY{n}{A7} \PY{o}{=} \PY{n}{d1}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{?}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{d4} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{d3}\PY{p}{)}
         \PY{n}{x\PYZus{}2} \PY{o}{=} \PY{n}{d4}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{d4}\PY{o}{.}\PY{n}{columns}\PY{o}{!=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A16}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{y\PYZus{}2} \PY{o}{=} \PY{n}{d4}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A16}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{n}{xTrain\PYZus{}2}\PY{p}{,} \PY{n}{xVal\PYZus{}2}\PY{p}{,} \PY{n}{yTrain\PYZus{}2}\PY{p}{,} \PY{n}{yVal\PYZus{}2} \PY{o}{=} \PY{n}{tts}\PY{p}{(}\PY{n}{x\PYZus{}2}\PY{p}{,} \PY{n}{y\PYZus{}2}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
\end{Verbatim}


    Comprobamos que el porcentaje medio obtenido se asemeja mucho a la ya
comprobada configuración de datos

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{meanTrainRegresion}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}42}]:} 84.87101449275363
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{curvaAprendizaje}\PY{p}{(}\PY{n}{xTrain\PYZus{}2}\PY{p}{,} \PY{n}{yTrain\PYZus{}2}\PY{p}{,} \PY{n}{xVal\PYZus{}2}\PY{p}{,} \PY{n}{yVal\PYZus{}2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_87_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{n}{l} \PY{o}{=} \PY{n}{ajustarLamb}\PY{p}{(}\PY{p}{)}
         \PY{n}{media2} \PY{o}{=} \PY{n}{meanTrainRegresion}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{l}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{media2}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}44}]:} 84.87826086956521
\end{Verbatim}
            
    Como vemos, una vez ajustada lambda para las dos configuraciones, se
obtienen unos resultados muy parecidos.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Atributos sustituidos con la moda: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{media1}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Atributos sustituidos con una clase nueva: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{media2}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Atributos sustituidos con la moda: 84.82028985507246
Atributos sustituidos con una clase nueva: 84.87826086956521

    \end{Verbatim}

    Usaremos aquella configuración de los datos que nos haya proporcionado
mejores resultados:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{k}{if}\PY{p}{(}\PY{n}{media2} \PY{o}{\PYZgt{}} \PY{n}{media1}\PY{p}{)}\PY{p}{:}
             \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal} \PY{o}{=} \PY{n}{x\PYZus{}2}\PY{p}{,} \PY{n}{y\PYZus{}2}\PY{p}{,} \PY{n}{xTrain\PYZus{}2}\PY{p}{,} \PY{n}{yTrain\PYZus{}2}\PY{p}{,} \PY{n}{xVal\PYZus{}2}\PY{p}{,} \PY{n}{yVal\PYZus{}2}
\end{Verbatim}


    Una vez tenemos los conjuntos de entrenamiento y validación procederemos
a analizar los 3 métodos de entrenamiento: \textbf{Regresión Logística},
\textbf{Redes Neuronales} y \textbf{Support Vector Machine}.

    \section{REGRESIÓN LOGÍSTICA}\label{regresiuxf3n-loguxedstica}

    Comenzamos con la regresión logística, este método requiere que
definamos las funciones necesarias para los cálculos matemáticos. El
método consiste en utilizar el gradiente de una función de costo para
encontrar el mínimo (global, a ser posible, aunque en ciertos casos
puede llevar a sobreajuste) de la función, para esto se utilizan unos
valores \(\theta\) que se multiplican por la matriz de entrenamiento
\(X\) y los cuales se van ajustando hasta hallar el mínimo.

Estas formulas ya fueron definidas en la entrada \emph{In {[}23{]}}, en
el apartado anterior, dado que se utilizó la regresión logística para
las pruebas. Sin embargo explicaremos las mismas:

Comenzamos con definir la función \emph{g}, que será nuestra función de
hipótesis y que responde a la siguiente fórmula:

\[g(z) = \frac{1}{1 + e^{-z}}\]

También necesitaremos la función de costo, definida matemáticamente
como:

\[J(\theta) = - \frac{1}{m}{( (log(g(X\theta) ))^{T}y) + (log(1 - g(X\theta)))^{T}(1-y)} + \frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j\]

Esta función la usaremos para buscar el mínimo, que será el punto en el
cual nuestro algoritmo esté optimizado. Para encontrar el mínimo será
necesaria la derivada de la función: el gradiente.

\[\frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m}{ X^{T} (g(X\theta) - y)} \quad  para\  j=0\]

\[\frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m}{ X^{T} (g(X\theta) - y)} + \frac{\lambda}{m}\theta_j \quad  para\  j\ge1\]

Además, añadiremos una función \emph{safe\_ln} que nos ayudará a evitar
que el logaritmo de \(1-g(X\theta)\) de error cuando \(g(X\theta)\) sea
1 en algún caso.

Para evitar problemas de overflow también definiremos una función
\emph{normalizar}, que normalizará los datos dados y devolverá la matriz
normalizada junto a la media y la desviación estandar utilizadas para la
misma.

    Vamos a entrenar con este método para obtener un porcentaje de acierto y
compararlo posteriormente con el resto de algoritmos. Además de obtener
el score también calcularemos el tiempo de ejecución, por lo que
usaremos la librería \emph{time} para ello:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{k+kn}{import} \PY{n+nn}{time}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{process\PYZus{}time}\PY{p}{(}\PY{p}{)}      
         
         \PY{n}{scoreRegresion} \PY{o}{=} \PY{n}{trainRegresion}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{n}{l}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{process\PYZus{}time}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{timeRegresion} \PY{o}{=} \PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Score: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{scoreRegresion}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tiempo de ejecución: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{timeRegresion}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Score: 89.13043478260869
Tiempo de ejecución: 0.078125

    \end{Verbatim}

    \section{REDES NEURONALES}\label{redes-neuronales}

    Seguiremos usando una red neuronal para clasificar nuestros datos. Este
método busca simular el comportamiento de las neuronas en el cerebro y
utiliza una serie de capas con un conjunto de nodos (neuronas) en cada
una, junto a unas matrices de pesos, para la clasificación. Las capas se
dividen en:\\
* \textbf{Capa de entrada}: Que recibirá los datos de entrada y, por
tanto, tendrá tantos nodos como variables tengan nuestros datos.\\
* \textbf{Capa oculta}: La cual, utilizando una matriz de pesos sobre la
capa de entrada, contendrá una transformación de los datos de entrada,
que serán una aproximación intermedia entre la entrada y la
clasificación final (la salida). Esta capa puede tener los nodos que se
deseen, pero lo más común es que se encuentren entre el tamaño de
entrada y de salida.

\begin{itemize}
\tightlist
\item
  \textbf{Cada de salida}: Una transformación de la capa oculta (a
  partir de otra matriz de pesos), que ofrece la clasificación final. La
  cantidad de nodos dependerá del modelo, si se trata de una
  clasificación multi-clase será necesario tener un nodo por clase,
  indicando la probabilidad de pertenencia a cada clase (aunque también
  se podría usar un solo nodo que indicase la clase con mayor
  probabilidad), mientras que en otro caso solo hará falta un nodo, ya
  sea para indicar la probabilidad de afirmativo/negativo (como en
  nuestro caso), como para obtener una predicción numérica.
\end{itemize}

La red neuronal puede tener múltiples capas ocultas (lo que se conoce
como deep-learning), y es una práctica necesaria en clasificaciones
complejas como reconocimiento de imágenes y de voz, sin embargo en
clasificaciones más simples como la nuestra bastará con una capa.

    El entrenamiento de la red neuronal es similar al de la regresión
logística: definimos unas funciones de coste y gradiente y minimizamos
las matrices de pesos (\(\theta s\)) mediante una función
\emph{minimize}.\\
Para entrenar la red neuronal necesitaremos definir nuevas funciones de
hipótesis, coste y gradiente que sean capaces de utilizar las matrices
de pesos.

    \subsection{HIPÓTESIS}\label{hipuxf3tesis}

    Es la encargada de crear la red neuronal y devolver un resultado:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{dg} \PY{o}{=} \PY{k}{lambda} \PY{n}{z}\PY{p}{:} \PY{n}{g}\PY{p}{(}\PY{n}{z}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{g}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
         \PY{n}{g} \PY{o}{=} \PY{k}{lambda} \PY{n}{z}\PY{p}{:} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{z}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{h}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
             \PY{n}{one} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{a1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{one}\PY{p}{,} \PY{n}{x}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{a2} \PY{o}{=} \PY{n}{g}\PY{p}{(}\PY{n}{a1}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{theta1}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{)}
             \PY{n}{a2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{one}\PY{p}{,} \PY{n}{a2}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{a3} \PY{o}{=} \PY{n}{g}\PY{p}{(}\PY{n}{a2}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{theta2}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{)} 
             
             \PY{k}{return} \PY{n}{a3}
\end{Verbatim}


    \subsection{FUNCIÓN DE COSTE}\label{funciuxf3n-de-coste}

    Dada por la siguiente expresión matemática:

\[J(\theta) = \frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}[  -y^{(i)}_k  log((h_\theta(x^{(i)}))_k) - (1 - y^{(i)}_k) log(1 - (h_\theta(x^{(i)}))_k)  ] \]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{k}{def} \PY{n+nf}{safe\PYZus{}ln}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{minval}\PY{o}{=}\PY{l+m+mf}{0.0000000001}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{clip}\PY{p}{(}\PY{n+nb}{min}\PY{o}{=}\PY{n}{minval}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{funCosteRed}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{)}\PY{p}{:}
             \PY{n}{yk} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{)}\PY{p}{)}
             \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{n}{yaux} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{y} \PY{o}{==} \PY{n}{k}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
                 \PY{n}{yk}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{yaux}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}
                 
             \PY{n}{h\PYZus{}result} \PY{o}{=} \PY{n}{h}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{x}\PY{p}{)}
             
             \PY{n}{yaux} \PY{o}{=} \PY{p}{(}\PY{n}{safe\PYZus{}ln}\PY{p}{(} \PY{n}{h\PYZus{}result}\PY{p}{)} \PY{o}{*} \PY{n}{yk}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{n}{safe\PYZus{}ln}\PY{p}{(} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{h\PYZus{}result} \PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{yk}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{return} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{*} \PY{n}{yaux}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Y su versión regularizada:

\[J(\theta) = \frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}[  -y^{(i)}_k   log((h_\theta(x^{(i)}))_k) - (1 - y^{(i)}_k)  log(1 - (h_\theta(x^{(i)}))_k)  ] + \frac{\lambda}{2m}[\sum_{j=1}^{N}\sum_{k=1}^{M}(\theta^{(1)}_{j,k})^{2} + \sum_{j=1}^{P}\sum_{k=1}^{Q}(\theta^{(2)}_{j,k})^{2} ] \]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{k}{def} \PY{n+nf}{funCosteReg}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}\PY{p}{:}    
             \PY{n}{coste} \PY{o}{=} \PY{n}{funCosteRed}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{)}    
                 
             \PY{k}{return} \PY{n}{coste} \PY{o}{+} \PY{p}{(}\PY{n}{lamb}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{theta1}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} 
                                               \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{theta2}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \subsection{GRADIENTE}\label{gradiente}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{k}{def} \PY{n+nf}{gradienteRed}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{)} \PY{p}{:} 
             \PY{n}{delta1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{theta1}\PY{p}{)}\PY{p}{,} \PY{n}{theta1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{size}\PY{p}{)}\PY{p}{)}
             \PY{n}{delta2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{theta2}\PY{p}{)}\PY{p}{,} \PY{n}{theta2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{size}\PY{p}{)}\PY{p}{)}
         
             
             \PY{n}{yk} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{)}\PY{p}{)}
             \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{n}{yaux} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{y} \PY{o}{==} \PY{n}{k}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
                 \PY{n}{yk}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{yaux}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}
                 
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{a1}\PY{p}{,} \PY{n}{a2}\PY{p}{,} \PY{n}{a3}\PY{p}{,} \PY{n}{z2} \PY{o}{=} \PY{n}{salidaCapas}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
         
                 \PY{n}{d3} \PY{o}{=} \PY{p}{(}\PY{n}{a3} \PY{o}{\PYZhy{}} \PY{n}{yk}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
                 
                 \PY{n}{z2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{z2}\PY{p}{)}
                 \PY{n}{d2} \PY{o}{=} \PY{p}{(}\PY{n}{theta2}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{d3}\PY{p}{)}\PY{o}{*}\PY{n}{dg}\PY{p}{(}\PY{n}{z2}\PY{p}{)}
             
                 \PY{n}{d2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{d2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
                 
                 \PY{n}{delta1} \PY{o}{+}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{d2}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}\PY{p}{,}\PY{n}{a1}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
                 \PY{n}{delta2} \PY{o}{+}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{d3}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}\PY{p}{,}\PY{n}{a2}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
                 
             \PY{n}{D1} \PY{o}{=} \PY{n}{delta1}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
             \PY{n}{D2} \PY{o}{=} \PY{n}{delta2}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{D1}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{D2}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    Para calcular el gradiente necesitaremos también la función
\emph{salidaCapas}, una redefinición de la función de hipótesis que nos
devolverá los valores de todas las capas junto a \emph{z2}, que es la
multiplicación de \(\theta^{(1)}x\), es decir la multiplicación de la
primera matriz de pesos (la que hay entre la capa de entrada y la
oculta) y los datos de entrada.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{k}{def} \PY{n+nf}{salidaCapas}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
             \PY{n}{a1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{x}\PY{p}{)}
             
             \PY{n}{z2} \PY{o}{=} \PY{n}{a1}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{theta1}\PY{o}{.}\PY{n}{T}\PY{p}{)}
             \PY{n}{a2} \PY{o}{=} \PY{n}{g}\PY{p}{(}\PY{n}{z2}\PY{p}{)}
         
             \PY{n}{a2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{a2}\PY{p}{)}
             
             \PY{n}{z3} \PY{o}{=} \PY{n}{a2}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{theta2}\PY{o}{.}\PY{n}{T}\PY{p}{)}
             \PY{n}{a3} \PY{o}{=} \PY{n}{g}\PY{p}{(}\PY{n}{z3}\PY{p}{)} 
             
             \PY{k}{return} \PY{p}{(}\PY{n}{a1}\PY{p}{,}\PY{n}{a2}\PY{p}{,}\PY{n}{a3}\PY{p}{,}\PY{n}{z2}\PY{p}{)}
\end{Verbatim}


    Dado que a las matrices de pesos se pasarán como parámetro mediante un
solo vector, necesitaremos una función \emph{despliega} que nos permita
separar ambas matrices. Este vector se debe a que las función
\emph{minimize} de \emph{scipy.optimize} solo permite minimizar un único
parametro, crear este vector nos da la posibilidad de optimizar los
pesos de multiples matrices como si fuesen un único parámetro.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{k}{def} \PY{n+nf}{despliega}\PY{p}{(}\PY{n}{params\PYZus{}rn}\PY{p}{,} \PY{n}{num\PYZus{}entradas}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{num\PYZus{}ocultas}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
             
             \PY{n}{theta1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape} \PY{p}{(} \PY{n}{params\PYZus{}rn} \PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}ocultas} \PY{o}{*} \PY{p}{(}\PY{n}{num\PYZus{}entradas} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{,}
             \PY{p}{(}\PY{n}{num\PYZus{}ocultas}\PY{p}{,} \PY{p}{(}\PY{n}{num\PYZus{}entradas} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{p}{)}
             
             \PY{n}{theta2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape} \PY{p}{(} \PY{n}{params\PYZus{}rn} \PY{p}{[}\PY{n}{num\PYZus{}ocultas} \PY{o}{*} \PY{p}{(}\PY{n}{num\PYZus{}entradas} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}\PY{p}{]}\PY{p}{,}
             \PY{p}{(}\PY{n}{num\PYZus{}etiquetas}\PY{p}{,} \PY{p}{(}\PY{n}{num\PYZus{}ocultas} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{p}{)}
             
             \PY{k}{return} \PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{)}
\end{Verbatim}


    Finalmente definimos \emph{costeRN}, que devuelve una tupla con el coste
y el gradiente, esta es la función que utilizará \emph{minimize} para
realizar los cálculos. Recibirá el vector de \(\theta s\) mediante el
parámetro \emph{params\_rn} y, mediante la función \emph{despliega} y
los tamaños de las capas lo convertirá en dos matrices.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{k}{def} \PY{n+nf}{costeRN}\PY{p}{(}\PY{n}{params\PYZus{}rn}\PY{p}{,} \PY{n}{num\PYZus{}entradas}\PY{p}{,} \PY{n}{num\PYZus{}ocultas}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{reg}\PY{p}{)} \PY{p}{:}
             \PY{n}{theta1}\PY{p}{,} \PY{n}{theta2} \PY{o}{=} \PY{n}{despliega}\PY{p}{(}\PY{n}{params\PYZus{}rn}\PY{p}{,} \PY{n}{num\PYZus{}entradas}\PY{p}{,} \PY{n}{num\PYZus{}ocultas}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{)}
         
             \PY{n}{coste} \PY{o}{=} \PY{n}{funCosteReg}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{,} \PY{n}{reg}\PY{p}{)}
             \PY{n}{gradient} \PY{o}{=} \PY{n}{gradienteRed}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{)}
             
             \PY{n}{delta1}\PY{p}{,} \PY{n}{delta2} \PY{o}{=} \PY{n}{despliega}\PY{p}{(}\PY{n}{gradient}\PY{p}{,} \PY{n}{num\PYZus{}entradas}\PY{p}{,} \PY{n}{num\PYZus{}ocultas}\PY{p}{,} \PY{n}{num\PYZus{}etiquetas}\PY{p}{)}
                         
             \PY{n}{delta1}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{theta1}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{*} \PY{n}{reg}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
             \PY{n}{delta2}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{theta2}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{*} \PY{n}{reg}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{gradient} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{delta1}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{delta2}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{return} \PY{p}{(}\PY{n}{coste}\PY{p}{,}\PY{n}{gradient}\PY{p}{)}
\end{Verbatim}


    Para entrenar la red neuronal inicializaremos las matrices de pesos con
unos valores aleatorios dentro del intervalo
(\(\epsilon_{ini}\))determinado por:
\[\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in}+{L_{out}}}}\]\\
Donde \(in\) es la capa que usa la matriz de pesos para transformarse en
\(out\) y \(L_{in}\) y \(L_{out}\) son los tamaños de dichas capas.\\
De este modo en cada entrenamiento las matrices iniciales serán
distintas, lo que nos permitirá encontrar distintos valores y quedarnos
con los que nos ofrezcan mejores resultados.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{k}{def} \PY{n+nf}{pesosAleatorios}\PY{p}{(}\PY{n}{L\PYZus{}in}\PY{p}{,} \PY{n}{L\PYZus{}out}\PY{p}{)}\PY{p}{:}
             \PY{n}{Eini} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{6}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mf}{2.0}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{L\PYZus{}in}\PY{o}{+}\PY{n}{L\PYZus{}out}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mf}{2.0}\PY{p}{)}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{Eini}\PY{p}{,} \PY{n}{Eini}\PY{p}{,} \PY{p}{(}\PY{n}{L\PYZus{}in}\PY{p}{,} \PY{n}{L\PYZus{}out}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    Para terminar, definimos una función \emph{trainRN} que será la
encargada de realizar el entrenamiento y la validación. Esta función
recibe el valor de regulación \emph{lamb}, el tamaño de las capas de
entrada y oculta en un vector/tupla y los datos de entrenamiento y
validación en una tupla, además también dispone de un parámetro para
seleccionar el número de iteraciones que se desean usar para el
entrenamiento (cuantas más iteraciones más se ajustará a los datos de
entrenamiento, pero podría generar sobreajuste).

La función devolverá una tupla con los porcentajes de acierto en la
validación y entrenamiento en una tupla y las matrices de pesos en un
vector.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{k}{def} \PY{n+nf}{trainRN}\PY{p}{(}\PY{n}{lamb}\PY{p}{,} \PY{n}{capas}\PY{p}{,} \PY{n}{datos}\PY{p}{,} \PY{n}{iteraciones}\PY{o}{=}\PY{l+m+mi}{70}\PY{p}{)}\PY{p}{:}
             \PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal} \PY{o}{=} \PY{n}{datos}
             \PY{n}{thetaRnd1} \PY{o}{=} \PY{n}{pesosAleatorios}\PY{p}{(}\PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{thetaRnd2} \PY{o}{=} \PY{n}{pesosAleatorios}\PY{p}{(}\PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{thetaRnd} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{r\PYZus{}}\PY{p}{[}\PY{n}{thetaRnd1}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{thetaRnd2}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}
             \PY{n}{xP} \PY{o}{=} \PY{n}{normalizar}\PY{p}{(}\PY{n}{xTrain}\PY{p}{)}
             \PY{n}{fmin} \PY{o}{=} \PY{n}{opt}\PY{o}{.}\PY{n}{minimize}\PY{p}{(}\PY{n}{costeRN}\PY{p}{,} \PY{n}{thetaRnd}\PY{p}{,} \PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                                 \PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{lamb}\PY{p}{)}\PY{p}{,} \PY{n}{method}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TNC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                 \PY{n}{options}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{maxiter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{iteraciones}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{jac}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             
             \PY{n}{optTheta} \PY{o}{=} \PY{n}{fmin}\PY{o}{.}\PY{n}{x}
            
             \PY{n}{theta1}\PY{p}{,} \PY{n}{theta2} \PY{o}{=} \PY{n}{despliega}\PY{p}{(}\PY{n}{optTheta}\PY{p}{,} \PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
             
             \PY{n}{xval} \PY{o}{=} \PY{p}{(}\PY{n}{xVal}\PY{o}{\PYZhy{}}\PY{n}{xP}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{/}\PY{n}{xP}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
             \PY{n}{probabilidades} \PY{o}{=} \PY{n}{h}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{xVal}\PY{p}{)} 
             \PY{n}{probabilidades} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1} \PY{k}{if} \PY{n}{i} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.5} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{probabilidades}\PY{p}{]}
             
             \PY{n}{correctos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{probabilidades} \PY{o}{==} \PY{n}{yVal}\PY{p}{)}
         
             \PY{n}{porcentajeVal} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{correctos}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{probabilidades}\PY{p}{)}
             
         
             \PY{n}{probabilidades} \PY{o}{=} \PY{n}{h}\PY{p}{(}\PY{n}{theta1}\PY{p}{,} \PY{n}{theta2}\PY{p}{,} \PY{n}{xTrain}\PY{p}{)}
             \PY{n}{probabilidades} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1} \PY{k}{if} \PY{n}{i} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.5} \PY{k}{else} \PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{probabilidades}\PY{p}{]}
         
             \PY{n}{correctos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{probabilidades} \PY{o}{==} \PY{n}{yTrain}\PY{p}{)}
         
             \PY{n}{porcentajeTrain} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{correctos}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{probabilidades}\PY{p}{)}
             
             \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{porcentajeVal}\PY{p}{,}\PY{n}{porcentajeTrain}\PY{p}{)}\PY{p}{,} \PY{n}{optTheta}\PY{p}{)}
\end{Verbatim}


    Una vez tenemos todas las funciones empezaremos el entrenamiento.
Empezaremos con un valor de regulación de \(0.1\) y \(500\) iteraciones.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{n}{capas} \PY{o}{=} \PY{p}{[}\PY{n}{xTrain}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}
         \PY{n}{datos} \PY{o}{=} \PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{)}
         \PY{n}{trainRN}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{capas}\PY{p}{,} \PY{n}{datos}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}58}]:} (84.05797101449275, 90.57971014492753)
\end{Verbatim}
            
    Para intentar mejorar este porcentaje analizaremos el porcentaje de
acierto generando las curvas de aprendizaje en base al número de
iteraciones mediante una función \emph{curvaIteraciones}. Esta función
nos devolverá el número de iteraciones óptimo para nuestro modelo.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{k}{def} \PY{n+nf}{curvaIteraciones} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{xval}\PY{p}{,} \PY{n}{yval}\PY{p}{,} \PY{n}{lamb}\PY{p}{,} \PY{n}{capas}\PY{p}{,}
                               \PY{n}{iteraciones}\PY{p}{,} \PY{n}{intervalo}\PY{o}{=}\PY{n+nb}{slice}\PY{p}{(}\PY{k+kc}{None}\PY{p}{)}\PY{p}{)} \PY{p}{:}
             \PY{n}{trainError} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{iteraciones}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{validationError} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{iteraciones}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{iteraciones}\PY{p}{)}\PY{p}{)}\PY{p}{:}    
                 \PY{n}{validationError}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{trainError}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{trainRN}\PY{p}{(}\PY{n}{lamb}\PY{p}{,} 
                                             \PY{n}{capas}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{xval}\PY{p}{,}\PY{n}{yval}\PY{p}{)}\PY{p}{,} \PY{n}{iteraciones}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                 
             \PY{n}{validationError} \PY{o}{/}\PY{o}{=} \PY{l+m+mi}{100}
             \PY{n}{trainError} \PY{o}{/}\PY{o}{=} \PY{l+m+mi}{100}  
                 
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{iteraciones}\PY{p}{,} \PY{n}{trainError}\PY{p}{[}\PY{n}{intervalo}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Entrenamiento}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{iteraciones}\PY{p}{,} \PY{n}{validationError}\PY{p}{[}\PY{n}{intervalo}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validación}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Número de iteraciones}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{iteraciones}\PY{p}{[}\PY{n}{validationError}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{n}{it} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{70}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{300}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{,} \PY{l+m+mi}{750}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{1250}\PY{p}{,} \PY{l+m+mi}{1500}\PY{p}{,} \PY{l+m+mi}{2000}\PY{p}{]}
         \PY{n}{bestIt} \PY{o}{=} \PY{n}{curvaIteraciones}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{capas}\PY{p}{,} \PY{n}{it}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_126_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{n}{bestIt}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}61}]:} 500
\end{Verbatim}
            
    Como se puede observar el porcentaje de acierto sobre los datos de
entrenamiento aumenta con el número de iteraciones, sin embargo, el
porcentaje de validación parece descender llegado un máximo de
iteraciones, esto significa que a partir de ese punto hay sobreajuste,
por lo que tendremos que usar un número de iteraciones menor.

Otro modo de mejorar el porcentaje de acierto podría ser
aumentar/disminuir el número de nodos de la capa oculta, para
comprobarlo crearemos otra función que nos muestre las curvas de
aprendizaje en base al número de nodos. Esta función nos devolverá el
mejor número de nodos internos para nuestro modelo.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{k}{def} \PY{n+nf}{curvaNodos}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{xval}\PY{p}{,} \PY{n}{yval}\PY{p}{,} \PY{n}{lamb}\PY{p}{,} \PY{n}{capas}\PY{p}{,} \PY{n}{iteraciones}\PY{p}{,} \PY{n}{nodos}\PY{p}{,}
                        \PY{n}{intervalo}\PY{o}{=}\PY{n+nb}{slice}\PY{p}{(}\PY{k+kc}{None}\PY{p}{)}\PY{p}{)} \PY{p}{:}
             \PY{n}{trainError} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nodos}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{validationError} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nodos}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{nodos}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{nodos}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                 \PY{n}{validationError}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{trainError}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{trainRN}\PY{p}{(}\PY{n}{lamb}\PY{p}{,} 
                                             \PY{n}{capas}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{xval}\PY{p}{,}\PY{n}{yval}\PY{p}{)}\PY{p}{,} \PY{n}{iteraciones}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                 
             \PY{n}{validationError} \PY{o}{/}\PY{o}{=} \PY{l+m+mi}{100}
             \PY{n}{trainError} \PY{o}{/}\PY{o}{=} \PY{l+m+mi}{100}  
                 
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nodos}\PY{p}{,} \PY{n}{trainError}\PY{p}{[}\PY{n}{intervalo}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Entrenamiento}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nodos}\PY{p}{,} \PY{n}{validationError}\PY{p}{[}\PY{n}{intervalo}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validación}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Número de nodos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{nodos}\PY{p}{[}\PY{n}{validationError}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{n}{nodos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
         \PY{n}{bestNodo} \PY{o}{=} \PY{n}{curvaNodos}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{capas}\PY{p}{,} \PY{n}{bestIt}\PY{p}{,} \PY{n}{nodos}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_130_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{n}{bestNodo}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}64}]:} 17
\end{Verbatim}
            
    Una vez tenemos el mejor número de nodos y el mejor número de
iteraciones, tenemos que encontrar el valor óptimo para el parámetro
\(\lambda\). Para ello implementamos la función \emph{mejorAprendizaje}.
Esta función realiza \emph{(iteraciones\_bucle x lamb.size)}
entrenamientos, para asegurarnos de encontrar el mejor resultado
posible, ya que con cada entrenamiento las \(\theta\)'s se generan de
manera aleatoria por lo que llegamos a diferentes resultados dependiendo
de estas. Esta función devuelve el mejor porcentaje de acierto, el
tiempo de ejecución del entrenamiento en cuestión y la \(\lambda\)
utilizada para ello.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{k}{def} \PY{n+nf}{mejorAprendizaje}\PY{p}{(}\PY{n}{iteraciones\PYZus{}bucle}\PY{p}{,} \PY{n}{lamb}\PY{p}{,} 
                              \PY{n}{iteraciones\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{capas}\PY{p}{,} \PY{n}{datos}\PY{p}{)} \PY{p}{:}
             \PY{n}{mejorTotal} \PY{o}{=} \PY{l+m+mi}{0}
             
             \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n}{lamb} \PY{p}{:}
                 \PY{n}{mejorP} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{iteraciones\PYZus{}bucle}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)} \PY{p}{:}
                     
                     \PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{process\PYZus{}time}\PY{p}{(}\PY{p}{)}
                     \PY{n}{ap} \PY{o}{=} \PY{n}{trainRN}\PY{p}{(}\PY{n}{l}\PY{p}{,} \PY{n}{capas}\PY{p}{,} \PY{n}{datos}\PY{p}{,} \PY{n}{iteraciones\PYZus{}aprendizaje}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                     \PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{process\PYZus{}time}\PY{p}{(}\PY{p}{)}
                     \PY{n}{timeRN} \PY{o}{=} \PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}
                     
                     \PY{k}{if} \PY{n}{mejorP} \PY{o}{\PYZlt{}} \PY{n}{ap}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{:}
                         \PY{n}{mejorP} \PY{o}{=} \PY{n}{ap}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                         \PY{n}{mejorParcial} \PY{o}{=} \PY{p}{(}\PY{n}{ap}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{timeRN}\PY{p}{,} \PY{n}{l}\PY{p}{)}
                 
                 \PY{k}{if} \PY{n}{mejorTotal} \PY{o}{\PYZlt{}} \PY{n}{mejorP} \PY{p}{:}
                     \PY{n}{mejorTotal} \PY{o}{=} \PY{n}{mejorP}
                     \PY{n}{mejor} \PY{o}{=} \PY{n}{mejorParcial}
                        
             \PY{k}{return} \PY{n}{mejor}
\end{Verbatim}


    Elegimos una secuencia de lambdas a probar y establecemos el número
óptimo de nodos previamente calculado.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{n}{lambdas} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.03}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}
         \PY{n}{capas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{bestNodo}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{n}{np}\PY{o}{.}\PY{n}{seterr}\PY{p}{(}\PY{n}{over}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{scoreRN}\PY{p}{,} \PY{n}{timeRN}\PY{p}{,} \PY{n}{lamb} \PY{o}{=} \PY{n}{mejorAprendizaje}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{lambdas}\PY{p}{,} \PY{n}{bestIt}\PY{p}{,} \PY{n}{capas}\PY{p}{,} \PY{n}{datos}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{n}{lamb}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}68}]:} 1
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Score: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{scoreRN}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tiempo de ejecución: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{timeRN}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Score: 91.30434782608695
Tiempo de ejecución: 48.296875

    \end{Verbatim}

    \section{SUPPORT VECTOR MACHINES}\label{support-vector-machines}

    Finalizaremos con la SVM (Support Vector Machines). La SVM es un
algoritmo de aprendizaje automático que realiza la clasificación
generando puntos en un espacio de n dimensiones, donde n es el número de
atributos del dataset, y separa los puntos mediante un hiperplano
buscando la mejor división de los datos.

Para esto usaremos la libreria de sklearn \emph{svm}. Tenemos que
obtener un clasificador mediante la clase SVC con un kernel y un valor
de regularización. Ajustaremos nuestros casos de entrenamiento mediante
la función \emph{fit} y, finalmente, gracias a la función \emph{score}
podremos validar nuestros casos de validación y obtener el porcentaje de
acierto.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} \PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{as} \PY{n+nn}{svm}
\end{Verbatim}


    \subsection{KERNEL}\label{kernel}

    El kernel escogido debe ser uno lineal ya que, como hemos estudiado en
la asignatura, es más recomendable usarlo frente uno Gaussiano en estos
conjuntos de datos en los que el número de ejemplos de entrenamiento es
pequeño y el número de variables es grande.

Recordamos que nuestro conjunto de datos ronda las 30 características
dependiendo de la configuración de variables categóricas establecida
finalmente

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} \PY{n}{x}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}71}]:} (690, 29)
\end{Verbatim}
            
    \subsection{\texorpdfstring{PARÁMETRO DE REGULACIÓN
\emph{C}}{PARÁMETRO DE REGULACIÓN C}}\label{paruxe1metro-de-regulaciuxf3n-c}

    Para conseguir un buen parámetro C de regulación, vamos a crear una
función que entrene nuestro modelo varias veces y se quede con el mejor
parámetro:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{k}{def} \PY{n+nf}{ajustarC}\PY{p}{(}\PY{n}{valores}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{xval}\PY{p}{,} \PY{n}{yval}\PY{p}{)}\PY{p}{:}
             \PY{n}{mejor} \PY{o}{=} \PY{l+m+mi}{0}
             
             \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{valores} \PY{p}{:}
                 \PY{n}{clf} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{n}{c}\PY{p}{)}
                 \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
                 \PY{n}{porcent} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{xval}\PY{p}{,} \PY{n}{yval}\PY{p}{)}
                 
                 \PY{k}{if}\PY{p}{(}\PY{n}{mejor} \PY{o}{\PYZlt{}} \PY{n}{porcent}\PY{p}{)} \PY{p}{:}
                     \PY{n}{mejor} \PY{o}{=} \PY{n}{porcent}
                     \PY{n}{tupla} \PY{o}{=} \PY{p}{(}\PY{n}{c}\PY{p}{,} \PY{n}{mejor}\PY{p}{)}
         
             \PY{k}{return} \PY{n}{tupla}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{n}{val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}
         \PY{n}{val}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}73}]:} array([ 0.01      ,  0.53578947,  1.06157895,  1.58736842,  2.11315789,
                 2.63894737,  3.16473684,  3.69052632,  4.21631579,  4.74210526,
                 5.26789474,  5.79368421,  6.31947368,  6.84526316,  7.37105263,
                 7.89684211,  8.42263158,  8.94842105,  9.47421053, 10.        ])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{n}{c} \PY{o}{=} \PY{n}{ajustarC}\PY{p}{(}\PY{n}{val}\PY{p}{,} \PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{,} \PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{process\PYZus{}time}\PY{p}{(}\PY{p}{)} 
         
         \PY{n}{clf} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{n}{c}\PY{p}{)}
         \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{xTrain}\PY{p}{,} \PY{n}{yTrain}\PY{p}{)}
         \PY{n}{scoreSVM} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{xVal}\PY{p}{,} \PY{n}{yVal}\PY{p}{)}
         \PY{n}{scoreSVM} \PY{o}{*}\PY{o}{=} \PY{l+m+mi}{100}
         
         \PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{process\PYZus{}time}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{timeSVM} \PY{o}{=} \PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Score: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{scoreSVM}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tiempo de ejecución: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{timeSVM}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Score: 89.85507246376811
Tiempo de ejecución: 0.84375

    \end{Verbatim}

    \section{CONCLUSIONES}\label{conclusiones}

    Para finalizar realizaremos una gráfica con el porcentaje de validación
y el tiempo de ejecución que se ha empleado para entrenar al modelo.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{k}{def} \PY{n+nf}{plotConclusion}\PY{p}{(}\PY{n}{tiempo}\PY{p}{,} \PY{n}{score}\PY{p}{)}\PY{p}{:}
             \PY{n}{grupos} \PY{o}{=} \PY{l+m+mi}{3}
             \PY{n}{means\PYZus{}time} \PY{o}{=} \PY{n}{tiempo}
             \PY{n}{means\PYZus{}score} \PY{o}{=} \PY{n}{score}
         
             \PY{n}{index} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{grupos}\PY{p}{)}
             \PY{n}{bar\PYZus{}width} \PY{o}{=} \PY{l+m+mf}{0.35}
             \PY{n}{opacity} \PY{o}{=} \PY{l+m+mf}{0.8}
         
             \PY{n}{rects1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{index}\PY{p}{,} \PY{n}{means\PYZus{}time}\PY{p}{,} \PY{n}{bar\PYZus{}width}\PY{p}{,} 
                              \PY{n}{alpha}\PY{o}{=}\PY{n}{opacity}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
             \PY{n}{rects2} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{index} \PY{o}{+} \PY{n}{bar\PYZus{}width}\PY{p}{,} \PY{n}{means\PYZus{}score}\PY{p}{,} \PY{n}{bar\PYZus{}width}\PY{p}{,} 
                              \PY{n}{alpha}\PY{o}{=}\PY{n}{opacity}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Models}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score by model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{index} \PY{o}{+} \PY{n}{bar\PYZus{}width}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RN}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SVM}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{y\PYZus{}pos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{101}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{n}{y\PYZus{}pos}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
         
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{n}{t} \PY{o}{=} \PY{p}{(}\PY{n}{timeRegresion}\PY{p}{,} \PY{n}{timeRN}\PY{p}{,} \PY{n}{timeSVM}\PY{p}{)}
         \PY{n}{s} \PY{o}{=} \PY{p}{(}\PY{n}{scoreRegresion}\PY{p}{,} \PY{n}{scoreRN}\PY{p}{,} \PY{n}{scoreSVM}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{n}{plotConclusion}\PY{p}{(}\PY{n}{t}\PY{p}{,} \PY{n}{s}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_155_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Como se puede ver el porcentaje de acierto de los 3 modelos es muy
similar, por lo que en este caso la elección del modelo no afectará
tanto al score sino al tiempo de entrenamiento.

Las conclusiones que podemos sacar respecto del tiempo de ejecución es
que la Regresión es tremendamente rápida, con un tiempo de ejecución
menor de 1 segundo. La Red Neuronal es bastante lenta, debido a la gran
cantidad de operaciones que realiza para el entrenamiento. Y la SVM es
bastante rápida también, aunque no tanto como la regresión. Además, la
rapidez de la SVM va a depender sobre todo del parametro C utilizado,
siendo más rápida cuanto menor sea este parámetro y ofreciendonos unos
resultados de manera no muy estable, ya que el mejor parámetro C
dependerá de la configuración de los datos de entrenamiento.

Por tanto, si queremos un entrenamiento rápido la mejor opción será la
Regresión Logística, pero si deseamos un buen rendimiento será mejor
usar la Red Neuronal, dado que nos permite ajustar más parámetros y
funciona aún mejor con datasets más complejos.

Por su parte la SVM no sería muy recomendable dado que ofrece un
resultado medio y el tiempo de ejecución es bastante inestable por lo ya
explicado.

    \section{BIBLIOGRAFÍA}\label{bibliografuxeda}

    \begin{itemize}
\tightlist
\item
  https://www.coursera.org/learn/machine-learning
\item
  https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce\\
\item
  https://www.analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/
\item
  https://github.com/ResidentMario/missingno
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
