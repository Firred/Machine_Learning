{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Alumnos**: *Adrián Ogáyar Sanchez y Arturo Barbero Pérez*\n",
    "\n",
    ">**Grupo**: *11*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTRENAMIENTO DE REDES NEURONALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías que van a ser necesarias durante el desarrollo de la práctica: La librería **displayData** es necesaría para poder hacer el graficado de los números contenidos en la matriz de datos. **NumPy** es la librería que nos permite realizar calculos entre matrices y vectores de manera de más eficiente gracias a la vectorización. Importamos además **loadmat** que se va a encargar de obtener los datos del fichero mat. La librería **checkNNGradients** contiene el código incluido con la práctica para poder realizar el chequeo de gradiente. Por último, utilizamos **Optimize** de SciPy, que nos permite obtener el vector θ optimizado para cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from displayData import displayData\n",
    "import numpy as np\n",
    "from checkNNGradients import checkNNGradients\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCIÓN DE COSTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos utilizados para esta práctica es el mismo que el de la práctica 3. Tenemos una matriz de orden 5000x400 donde cada fila contiene un vector de 400 elementos que representa una imagen. Esta imagen en realidad es una matriz de 20x20 pixeles, pero se ha desplegado para poder operar con esta matriz de una manera óptima.\n",
    "El vector *y* contiene 5000 componentes que representan las etiquetas de cada ejemplo de entrenamiento. El \"0\" se ha representado como un \"10\", manteniendo las etiquetas naturales del \"1\" al \"9\" para el resto de números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X: (5000, 400)\n",
      "Forma de y: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "\n",
    "# se pueden consultar las claves con data.keys()\n",
    "y = data['y']\n",
    "X = data['X']\n",
    "# almacena los datos leídos en X, y\n",
    "\n",
    "print(\"Forma de X: \" + str(X.shape))\n",
    "print(\"Forma de y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver que los datos se han cargado correctamente podemos elegir 100 imagenes aleatoriamente de la matriz de datos y obtener una muestra de ellos mediante la función *displayData*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = np.random.choice(X.shape[0], 100)\n",
    "#fix, ax = displayData(X[sample, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal que vamos a utilizar tiene una estructura formada por tres capas, con 400 unidades en la capa de entrada, 25 en la oculta y 10 en la de salida.\n",
    "El fichero *ex4weights.mat* contiene las matrices $\\theta$ que podremos usar para comprobar si se realiza de manera correcta el cálculo del coste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de theta1: (25, 401)\n",
      "Forma de theta2: (10, 26)\n",
      "Forma de theta vectorizada: (10285,)\n"
     ]
    }
   ],
   "source": [
    "weights = loadmat(\"ex4weights.mat\")\n",
    "theta1, theta2 = weights[\"Theta1\"], weights[\"Theta2\"]\n",
    "theta = np.r_[theta1.ravel(), theta2.ravel()]\n",
    "\n",
    "print(\"Forma de theta1: \" + str(theta1.shape))\n",
    "print(\"Forma de theta2: \" + str(theta2.shape))\n",
    "print(\"Forma de theta vectorizada: \" + str(theta.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como para esta práctica, operaremos con la matriz $\\theta$ como si fuera un vector, declaramos además una función que se encargue de desplegar el vector $\\theta$ en las dos matrices correspondientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def despliega(params_rn, num_entradas=400, num_ocultas=25, num_etiquetas=10):\n",
    "    \n",
    "    theta1 = np.reshape ( params_rn [:num_ocultas * (num_entradas + 1)],\n",
    "    (num_ocultas, (num_entradas + 1)) )\n",
    "    \n",
    "    theta2 = np.reshape ( params_rn [num_ocultas * (num_entradas + 1):],\n",
    "    (num_etiquetas, (num_ocultas + 1)) )\n",
    "    \n",
    "    return (theta1, theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos, por tanto, a definir la función que se encargue de calcular el coste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos primero la función sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda z: 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En segundo lugar implementamos la función hipótesis que es la encargada de crear la red neuronal de 3 capas y devolver un resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta, x):\n",
    "    theta1, theta2 = despliega(theta)\n",
    "    \n",
    "    #La matriz X son las unidades pertenecientes a la capa de entrada\n",
    "    one = np.ones((len(x), 1))\n",
    "    #Añadimos a la matriz X la columna de 1's para obtener las unidades de la capa 1\n",
    "    a1 = np.hstack([one, x])\n",
    "    \n",
    "    #Obtenemos un resultado para las unidades de la capa oculta gracias a theta1\n",
    "    a2 = g(a1.dot(theta1.T))\n",
    "    #Añadimos la columna de 1's a la segunda capa\n",
    "    a2 = np.hstack([one, a2])\n",
    "    \n",
    "    #Obtenemos un resultado para las unidades de la capa de salida gracias a theta2\n",
    "    a3 = g(a2.dot(theta2.T)) \n",
    "    \n",
    "    return a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de coste viene dado por la siguiente expresión:\n",
    "\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[  -y^{(i)}_k  log((h_\\theta(x^{(i)}))_k) - (1 - y^{(i)}_k) log(1 - (h_\\theta(x^{(i)}))_k)  ] $$\n",
    "\n",
    "La cual se puede definir de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funCoste(theta, x, y, num_etiquetas):\n",
    "    \n",
    "    #Creamos la matriz yk de orden (5000, 10) que contiene una fila con todos los elementos\n",
    "    # a 0 salvo uno a 1 que indica el número a clasificar\n",
    "    yk = np.zeros((len(x), num_etiquetas))\n",
    "    for k in range(1, num_etiquetas + 1):\n",
    "        yaux = np.where(y == k, 1, 0)\n",
    "        yk[:, k-1] = yaux.ravel()\n",
    "       \n",
    "    #Calculamos la formula\n",
    "    yaux = (np.log( h(theta, x) ) * yk) + (np.log( 1 - h(theta, x) ) * (1-yk))\n",
    "\n",
    "    return -1/len(x) * yaux.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que el método funciona correctamente la probaremos con *num_etiquetas = 10*. El resultado obtenido debería estar entorno a: *0.287629*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funCoste(theta, X, y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación añadimos a la función de coste el termino de regularización, expresado con la siguiente fórmula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[  -y^{(i)}_k · log((h_\\theta(x^{(i)}))_k) - (1 - y^{(i)}_k)· log(1 - (h_\\theta(x^{(i)}))_k)  ] + \\frac{\\lambda}{2m}[\\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\theta^{(1)}_{j,k})^{2} + \\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\theta^{(2)}_{j,k})^{2} ] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funCosteReg(theta, x, y, num_etiquetas, lamb):\n",
    "    theta1, theta2 = despliega(theta)\n",
    "    \n",
    "    return ( funCoste(theta, X, y, 10) + (lamb/(2*len(X)))*((theta1**2).sum() + (theta2**2).sum()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, aunque en la fórmula se hayan dado los límites explícitos de $\\theta^{(1)}$ y $\\theta^{(2)}$, en esta versión vectorizada sirve para cualquier tipo de theta, ya que no se utilizan bucles que operen hasta un límite determinado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta función el resultado (con *$\\lambda$ = 1*) debería dar un valor aproximado a: *0.383770*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funCosteReg(theta, X, y, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CÁLCULO DEL GRADIENTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta parte de la práctica, vamos a implementar el algoritmo de retro-propagación para poder realizar el cálculo del gradiente. \n",
    "Implementamos una función auxiliar que calcula la derivada de la función sigmoide *g*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = lambda z: g(z) * (1 - g(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, implementamos una función que se encargue de inicializar una matriz de pesos $\\theta$ con valores aleatorios en un rango [$\\in_{ini}, \\in_{ini}$]. Utilizaremos el valor $\\in_{ini}$ = 0.12 para la inicialización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pesosAleatorios(L_in, L_out):\n",
    "    return np.random.uniform(-0.12, 0.12, (L_in, L_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retro-propagación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de retro-propagación permite calcular el gradiente de la función de coste de la red neuronal. Para ello, hay que ejecutar primero una pasada \"hacia delante\" para cada ejemplo de entrenamiento, para así calcular la salida de la red $h_\\theta(x)$. Una vez hecho esto, hacemos una pasada \"hacia atras\" para calcular en cada nodo *j* de cada capa *l* el error $\\delta^{(l)}_j$ que hay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def salidaCapas(theta, x):\n",
    "    theta1, theta2 = theta\n",
    "    \n",
    "    #Añadimos a la matriz X la columna de 1's para obtener las unidades de la capa 1\n",
    "    a1 = np.append(1, x)\n",
    "    \n",
    "    z2 = a1.dot(theta1.T)\n",
    "    #Obtenemos un resultado para las unidades de la capa oculta gracias a theta1\n",
    "    a2 = g(z2)\n",
    "\n",
    "    #Añadimos la columna de 1's a la segunda capa\n",
    "    a2 = np.append(1, a2)\n",
    "    \n",
    "    z3 = a2.dot(theta2.T)\n",
    "    #Obtenemos un resultado para las unidades de la capa de salida gracias a theta2\n",
    "    a3 = g(z3) \n",
    "    \n",
    "    return (a2,a3,z2)"
=======
    "def salidaCapas(theta, x)\n"
>>>>>>> arturo
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def gradiente(x, y, theta) : \n",
    "    np.seterr(all='warn')\n",
    "    theta1, theta2 = theta\n",
    "   \n",
    "    theta2 = np.delete(theta2, 0, 1)\n",
    "    \n",
    "    delta1 = np.zeros((len(theta1),x[0].size))\n",
    "    delta2 = np.zeros((len(theta2),len(theta1)))\n",
    "    \n",
    "    yk = np.zeros((len(x), 10))\n",
    "    for k in range(1, 11):\n",
    "        yaux = np.where(y == k, 1, 0)\n",
    "        yk[:, k-1] = yaux.ravel()\n",
    "    \n",
    "    for i in range(0, len(x)) :\n",
    "        xaux = x[i,:]\n",
    "\n",
    "        s = salidaCapas(theta, xaux)\n",
    "\n",
    "        d3 = (s[1] - yk[i])\n",
    "        \n",
    "        a2 = np.delete(s[0], 0)\n",
    "        \n",
    "        d2 = theta2.T.dot(d3.T)*dg(s[2].T)\n",
    "        \n",
    "        delta1+=delta1+np.matmul(d2[:,np.newaxis],xaux[np.newaxis,:])\n",
    "        delta2+=delta2+np.matmul(d3[:,np.newaxis],a2[np.newaxis,:])\n",
    "        \n",
    "        delta1 = delta1/len(x)\n",
    "        delta2 = delta2/len(x)\n",
    "        \n",
    "        return (delta1,delta2)"
=======
    "def gradiente(X, y, theta)\n"
>>>>>>> arturo
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar todo esto definiremos una función que se encargue de calcular el coste y vector gradiente de la red neuronal, dados los siguientes parámetros"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": null,
>>>>>>> arturo
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "    \n",
    "    theta1, theta2 = despliega(params_rn, num_entradas, num_ocultas, num_etiquetas)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (coste, gradiente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función devuelve una tupla, siendo el primer elemento el coste y el segundo el vector gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chequeo del gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que el gradiente se ha calculado de forma correcta vamos a utilizar el fichero proporcionado con la práctica *checkNNGradients.py* que contiene una función que aproxima el valor de la derivada por este método. Utilizaremos la función con el mismo nombre que el fichero, ya que contiene una red neuronal que facilita los cálculos. En este método se aplican los dos métodos de cálculo del gradiente, el numérico y el que hemos implementado mediante la función *backprop*, para poder comparar sus resultados. Si el gradiente está bien implementado, la diferencia debería ser menor de $10^9$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(checkNNGradients(backprop, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
