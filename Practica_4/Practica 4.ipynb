{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Alumnos**: *Adrián Ogáyar Sanchez y Arturo Barbero Pérez*\n",
    "\n",
    ">**Grupo**: *11*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTRENAMIENTO DE REDES NEURONALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías que van a ser necesarias durante el desarrollo de la práctica: La librería **displayData** es necesaría para poder hacer el graficado de los números contenidos en la matriz de datos. **NumPy** es la librería que nos permite realizar calculos entre matrices y vectores de manera de más eficiente gracias a la vectorización. Importamos además **loadmat** que se va a encargar de obtener los datos del fichero mat. Por último, utilizamos **Optimize** de SciPy, que nos permite obtener el vector θ optimizado para cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from displayData import displayData\n",
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCIÓN DE COSTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos utilizados para esta práctica es el mismo que el de la práctica 3. Tenemos una matriz de orden 5000x400 donde cada fila contiene un vector de 400 elementos que representa una imagen. Esta imagen en realidad es una matriz de 20x20 pixeles, pero se ha desplegado para poder operar con esta matriz de una manera óptima.\n",
    "El vector *y* contiene 5000 componentes que representan las etiquetas de cada ejemplo de entrenamiento. El \"0\" se ha representado como un \"10\", manteniendo las etiquetas naturales del \"1\" al \"9\" para el resto de números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "\n",
    "# se pueden consultar las claves con data.keys()\n",
    "y = data['y']\n",
    "X = data['X']\n",
    "# almacena los datos leídos en X, y\n",
    "\n",
    "print(\"Forma de X: \" + str(X.shape))\n",
    "print(\"Forma de y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver que los datos se han cargado correctamente podemos elegir 100 imagenes aleatoriamente de la matriz de datos y obtener una muestra de ellos mediante la función *displayData*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(X.shape[0], 100)\n",
    "fix, ax = displayData(X[sample, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal que vamos a utilizar tiene una estructura formada por tres capas, con 400 unidades en la capa de entrada, 25 en la oculta y 10 en la de salida.\n",
    "El fichero *ex4weights.mat* contiene las matrices $\\theta$ que podremos usar para comprobar si se realiza de manera correcta el cálculo del coste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = loadmat(\"ex4weights.mat\")\n",
    "theta1, theta2 = weights[\"Theta1\"], weights[\"Theta2\"]\n",
    "\n",
    "print(\"Forma de theta1: \" + str(theta1.shape))\n",
    "print(\"Forma de theta2: \" + str(theta2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos, por tanto, a definir la función que se encargue de calcular el coste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos primero la función sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda z: 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En segundo lugar implementamos la función hipótesis que es la encargada de crear la red neuronal de 3 capas y devolver un resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta, x):\n",
    "    theta1, theta2 = theta\n",
    "    \n",
    "    #La matriz X son las unidades pertenecientes a la capa de entrada\n",
    "    one = np.ones((len(x), 1))\n",
    "    #Añadimos a la matriz X la columna de 1's para obtener las unidades de la capa 1\n",
    "    a1 = np.hstack([one, x])\n",
    "    \n",
    "    #Obtenemos un resultado para las unidades de la capa oculta gracias a theta1\n",
    "    a2 = g(a1.dot(theta1.T))\n",
    "    #Añadimos la columna de 1's a la segunda capa\n",
    "    a2 = np.hstack([one, a2])\n",
    "    \n",
    "    #Obtenemos un resultado para las unidades de la capa de salida gracias a theta2\n",
    "    a3 = g(a2.dot(theta2.T)) \n",
    "    \n",
    "    return a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de coste viene dado por la siguiente expresión:\n",
    "\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[  -y^{(i)}_k  log((h_\\theta(x^{(i)}))_k) - (1 - y^{(i)}_k) log(1 - (h_\\theta(x^{(i)}))_k)  ] $$\n",
    "\n",
    "La cual se puede definir de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funCoste(theta, x, y, num_etiquetas):\n",
    "    \n",
    "    #Creamos la matriz yk de orden (5000, 10) que contiene una fila con todos los elementos\n",
    "    # a 0 salvo uno a 1 que indica el número a clasificar\n",
    "    yk = np.zeros((len(x), num_etiquetas))\n",
    "    for k in range(1, num_etiquetas + 1):\n",
    "        yaux = np.where(y == k, 1, 0)\n",
    "        yk[:, k-1] = yaux.ravel()\n",
    "       \n",
    "    #Calculamos la formula\n",
    "    yaux = (np.log( h(theta, x) ) * yk) + (np.log( 1 - h(theta, x) ) * (1-yk))\n",
    "\n",
    "    return -1/len(x) * yaux.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que el método funciona correctamente la probaremos con *num_etiquetas = 10*. El resultado obtenido debería estar entorno a: *0.287629*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funCoste((theta1,theta2), X, y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación añadimos a la función de coste el termino de regularización, expresado con la siguiente fórmula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[  -y^{(i)}_k · log((h_\\theta(x^{(i)}))_k) - (1 - y^{(i)}_k)· log(1 - (h_\\theta(x^{(i)}))_k)  ] + \\frac{\\lambda}{2m}[\\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\theta^{(1)}_{j,k})^{2} + \\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\theta^{(2)}_{j,k})^{2} ] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funCosteReg(theta, x, y, num_etiquetas, lamb):\n",
    "    theta1, theta2 = theta\n",
    "    \n",
    "    return ( funCoste(theta, X, y, 10) + (lamb/(2*len(X)))*((theta1**2).sum() + (theta2**2).sum()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, aunque en la fórmula se hayan dado los límites explícitos de $\\theta^{(1)}$ y $\\theta^{(2)}$, en esta versión vectorizada sirve para cualquier tipo de theta, ya que no se utilizan bucles que operen hasta un límite determinado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta función el resultado (con *$\\lambda$ = 1*) debería dar un valor aproximado a: *0.383770*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funCosteReg((theta1, theta2), X, y, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CÁLCULO DEL GRADIENTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta parte de la práctica, vamos a implementar el algoritmo de retro-propagación para poder realizar el cálculo del gradiente. \n",
    "Implementamos una función auxiliar que calcula la derivada de la función sigmoide *g*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = lambda z: g(z) * (1 - g(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, implementamos una función que se encargue de inicializar una matriz de pesos $\\theta$ con valores aleatorios en un rango [$\\in_ini, \\in_ini$]. Utilizaremos el valor $\\in_ini$ = 0.12 para la inicialización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesosAleatorios(L_in, L_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar todo esto definiremos una función que se encargue de calcular el coste y vector gradiente de la red neuronal, dados los siguientes parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "    \n",
    "    theta1 = np.reshape ( params_rn [:num_ocultas * (num_entradas + 1)],\n",
    "    (num_ocultas, (num_entradas + 1)) )\n",
    "    \n",
    "    theta2 = np.reshape ( params_rn [num_ocultas * (num_entradas + 1):],\n",
    "    (num_etiquetas, (num_ocultas + 1)) )\n",
    "    \n",
    "    theta = (theta1, theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función devuelve una tupla, siendo el primer elemento el coste y el segundo el vector gradiente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
